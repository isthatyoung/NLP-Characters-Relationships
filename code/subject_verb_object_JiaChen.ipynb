{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze subject and object in setence demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk.data\n",
    "import chardet\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "import nltk.data\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import spacy\n",
    "from tabulate import tabulate\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function for generating pre-treated data \n",
    "def main():\n",
    "    #dir = \"/Users/chenjia/study/nlp/code/group_project/text_data/\"\n",
    "    dir = \"/Users/isthatyoung/temp/NLP-Characters-Relationships/text_data/\"\n",
    "    raw_text_data = read_file(dir)\n",
    "    valid_data = pre_processing(raw_text_data)\n",
    "    return valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file # same as siyang's code\n",
    "def read_file(dir):\n",
    "    json_file = Path('{}Raw-Harry-Potter-Series.json'.format(dir))\n",
    "    book = {}\n",
    "    if not json_file.exists():\n",
    "        files = []\n",
    "        pattern = '\\.txt'\n",
    "        for readfile in os.listdir(dir):\n",
    "            if re.search(pattern, readfile):\n",
    "                files.append(readfile)\n",
    "        pattern = '(.+)\\..+$'\n",
    "        for file_name in files:\n",
    "            decode_text = []\n",
    "            with open(dir + file_name, 'rb') as f_read:\n",
    "                text = f_read.readlines()\n",
    "                for line in text:\n",
    "                    type = chardet.detect(line)\n",
    "                    line = line.decode(type[\"encoding\"])\n",
    "                    decode_text.append(line)\n",
    "                file_name = re.findall(pattern, file_name)[0]\n",
    "                book[file_name] = decode_text\n",
    "            f_read.close()\n",
    "\n",
    "        ##TODO:Save book to json file\n",
    "        with open(dir + 'Raw-Harry-Potter-Series.json', 'a') as outfile:\n",
    "            json.dump(book, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "    else:\n",
    "        readfile = open('{}Raw-Harry-Potter-Series.json'.format(dir),\"r\")\n",
    "        book = eval(readfile.read())\n",
    "    return book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing: we need each novel to be tokenized into sentences, not a whole novel, not tokens.\n",
    "### 此方法断句会产生断开Mr.Weasley的错误，但也不是不能用。 当前主函数没有调用这个。###\n",
    "\n",
    "def preprocessing(data):\n",
    "    \n",
    "    data_into_sentences = {}\n",
    "    \n",
    "    for key in data.keys():\n",
    "        text_of_book = data[key]\n",
    "       \n",
    "        pattern = r'^[¡]*'\n",
    "        \n",
    "        new_sentence = []\n",
    "        for sentence in text_of_book:\n",
    "            sentence = sentence.strip('\\n')\n",
    "            sentence = re.sub(pattern, '', sentence)\n",
    "            new_sentence.append(sentence)\n",
    "            \n",
    "        a = sent_tokenize(str(new_sentence))\n",
    "        data_into_sentences[key] = a\n",
    "\n",
    "\n",
    "    return data_into_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing: we need each novel to be tokenized into sentences, not a whole novel, not tokens.\n",
    "### 此方法断句避免了断开Mr. Weasley的问题，可用。 当前主函数调用的是该函数。###\n",
    "def pre_processing(data):\n",
    "    \n",
    "    data_into_sentences = {}\n",
    "    \n",
    "    for key in data.keys():\n",
    "        text_of_book = data[key]\n",
    "       \n",
    "        pattern = r'^[¡]*'\n",
    "        \n",
    "        all_paragraph = []\n",
    "        for para in text_of_book:\n",
    "            para = para.strip('\\n')\n",
    "            para = re.sub(pattern, '', para)\n",
    "            all_paragraph.append(para)\n",
    "        \n",
    "        all_sentence = []\n",
    "        for i in all_paragraph:\n",
    "            for j in sent_tokenize(i):\n",
    "                all_sentence.append(j)\n",
    "\n",
    "        data_into_sentences[key] = all_sentence\n",
    "\n",
    "\n",
    "    return data_into_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run main function and generate valid data\n",
    "valid_data = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Harry Potter and the Deathly Hallows', 'Harry Potter and the Prisoner of Azkaban', 'Harry Potter and the Order of the Phoenix', 'Harry Potter and the Sorcerer Stone', 'Harry Potter and The Half-Blood Prince', 'Harry Potter and The Chamber Of Secrets', 'Harry Potter and the Goblet of Fire'])\n"
     ]
    }
   ],
   "source": [
    "# See what valid_data looks like and check whether there is problem\n",
    "print(valid_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHAPTER ONE\n"
     ]
    }
   ],
   "source": [
    "print(valid_data['Harry Potter and the Sorcerer Stone'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE BOY WHO LIVED\n"
     ]
    }
   ],
   "source": [
    "print(valid_data['Harry Potter and the Sorcerer Stone'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much.\n"
     ]
    }
   ],
   "source": [
    "print(valid_data['Harry Potter and the Sorcerer Stone'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense.\n"
     ]
    }
   ],
   "source": [
    "print(valid_data['Harry Potter and the Sorcerer Stone'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the valid_data into content_json format as required # almost same as Janine's code\n",
    "content_json = []\n",
    "verse = 0\n",
    "#book1\n",
    "for sentence in valid_data['Harry Potter and the Sorcerer Stone']:\n",
    "    verse = verse + 1\n",
    "    chunk = {\"book_id\":1}\n",
    "    chunk[\"text\"] = sentence\n",
    "    chunk[\"verse\"] = verse\n",
    "    content_json.append(chunk)\n",
    "#book2\n",
    "for sentence in valid_data['Harry Potter and The Chamber Of Secrets']:\n",
    "    verse = verse+1\n",
    "    chunk = {\"book_id\":2}\n",
    "    chunk[\"text\"] = sentence\n",
    "    chunk[\"verse\"] = verse\n",
    "    content_json.append(chunk)\n",
    "#book3\n",
    "for sentence in valid_data['Harry Potter and the Prisoner of Azkaban']:\n",
    "    verse = verse+1\n",
    "    chunk = {\"book_id\":3}\n",
    "    chunk[\"text\"] = sentence\n",
    "    chunk[\"verse\"] = verse\n",
    "    content_json.append(chunk)\n",
    "#book4\n",
    "for sentence in valid_data['Harry Potter and the Goblet of Fire']:\n",
    "    verse = verse+1\n",
    "    chunk = {\"book_id\":4}\n",
    "    chunk[\"text\"] = sentence\n",
    "    chunk[\"verse\"] = verse\n",
    "    content_json.append(chunk)\n",
    "#book5\n",
    "for sentence in valid_data['Harry Potter and the Order of the Phoenix']:\n",
    "    verse = verse+1\n",
    "    chunk = {\"book_id\":5}\n",
    "    chunk[\"text\"] = sentence\n",
    "    chunk[\"verse\"] = verse\n",
    "    content_json.append(chunk)\n",
    "#book6\n",
    "for sentence in valid_data['Harry Potter and The Half-Blood Prince']:\n",
    "    verse = verse+1\n",
    "    chunk = {\"book_id\":6}\n",
    "    chunk[\"text\"] = sentence\n",
    "    chunk[\"verse\"] = verse\n",
    "    content_json.append(chunk)\n",
    "#book7\n",
    "for sentence in valid_data['Harry Potter and the Deathly Hallows']:\n",
    "    verse = verse+1\n",
    "    chunk = {\"book_id\":7}\n",
    "    chunk[\"text\"] = sentence\n",
    "    chunk[\"verse\"] = verse\n",
    "    content_json.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "---\n",
      "[{'book_id': 1, 'text': 'CHAPTER ONE', 'verse': 1}, {'book_id': 1, 'text': 'THE BOY WHO LIVED', 'verse': 2}]\n"
     ]
    }
   ],
   "source": [
    "# See what the content_json looks like\n",
    "print(type(content_json))\n",
    "print('---')\n",
    "print(content_json[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final_data for next step, namely find subject, verb, and object\n",
    "final_data = []\n",
    "for i in range(len(content_json)):\n",
    "    final_data.append(content_json[i][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He'd forgotten all about the people in cloaks until he passed a group of them next to the baker's.\n"
     ]
    }
   ],
   "source": [
    "# See what the final_data looks like\n",
    "print(final_data[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2994\n"
     ]
    }
   ],
   "source": [
    "# Find subject + verb + object!!! \n",
    "### Time complexity is n. time: 18min\n",
    "\n",
    "### Result using data produced by preprocessing（）:\n",
    "# subject + verb: 24057\n",
    "# subject + verb + object (helper1 = helper2 = 1): 3222\n",
    "# subject + verb + object (helper1 >= helper2 >= 1): 4582\n",
    "###\n",
    "\n",
    "### Result using data produced by pre_processing（）:\n",
    "# subject + verb + object (helper1 = helper2 = 1): 2994\n",
    "# 其他还没试\n",
    "####\n",
    "\n",
    "### Corner case （可以先不管）\n",
    "'''\n",
    "Mrs. Potter was Mrs. Dursley's sister, but they hadn't met for several years; \n",
    "in fact, Mrs. Dursley pretended she didn't have a sister, because her sister and\n",
    "her good-for-nothing husband were as unDursleyish as it was possible to be.\n",
    "'''\n",
    "\n",
    "\n",
    "sub_verb_obj = []\n",
    "\n",
    "def token_is_subject_with_action(token):\n",
    "    nsubj = token.dep_ == 'nsubj'\n",
    "    head_verb = token.head.pos_ == 'VERB'\n",
    "    person = token.ent_type_ == 'PERSON'\n",
    "    return nsubj and head_verb and person\n",
    "\n",
    "def token_is_dobject(token):\n",
    "    dobj = token.dep_ == 'dobj'\n",
    "    person = token.ent_type_ == 'PERSON'    \n",
    "    return  dobj and person                 \n",
    "\n",
    "def token_is_pobject(token):\n",
    "    pobj = token.dep_ == 'pobj'\n",
    "    person = token.ent_type_ == 'PERSON'    \n",
    "    return  pobj and person\n",
    "\n",
    "\n",
    "for i in range(len(final_data)):\n",
    "    doc = nlp(final_data[i])\n",
    "    helper_nsubj, helper_obj = 0, 0 \n",
    "    \n",
    "    for token in doc:\n",
    "        if token_is_subject_with_action(token):\n",
    "            helper_nsubj += 1\n",
    "            token_nsubj = token\n",
    "            \n",
    "        elif token_is_dobject(token) or token_is_pobject(token):\n",
    "            helper_obj += 1\n",
    "            token_obj = token\n",
    "    \n",
    "    # 此处可以有多种修改模式，当前条件是一句话里有一个主语和一个宾语\n",
    "    # 可以改成一句话里至少有一个主语和一个宾语\n",
    "    # 也可以改成两个helper相加为2，等多种方法，比较哪种方法比较好\n",
    "    if helper_nsubj == 1 and helper_obj == 1:                    \n",
    "        span = doc[token_nsubj.head.left_edge.i:token_nsubj.head.right_edge.i+1]\n",
    "        # print(span)\n",
    "        data = dict(\n",
    "                    name = token_nsubj.orth_,\n",
    "                    span = span.text,\n",
    "                    verb = token_nsubj.head.lower_,\n",
    "                    log_prob = token_nsubj.head.prob,\n",
    "                    obj = token_obj.orth_\n",
    "                    )\n",
    "\n",
    "        sub_verb_obj.append(data)\n",
    "            \n",
    "\n",
    "print(len(sub_verb_obj)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Voldemort', 'span': \"that last night Voldemort turned up in Godric's Hollow\", 'verb': 'turned', 'log_prob': -9.319558143615723, 'obj': 'Hollow'}\n"
     ]
    }
   ],
   "source": [
    "# See what the result looks like\n",
    "print(sub_verb_obj[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Petunia', 'span': 'After asking Harry furiously if he knew the man, Aunt Petunia had rushed them out of the shop without buying anything.', 'verb': 'rushed', 'log_prob': -11.93223762512207, 'obj': 'Harry'}\n"
     ]
    }
   ],
   "source": [
    "print(sub_verb_obj[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Dumbledore', 'span': 'I would trust Hagrid with my life,\" said Dumbledore.', 'verb': 'said', 'log_prob': -7.258025169372559, 'obj': 'Hagrid'}\n"
     ]
    }
   ],
   "source": [
    "print(sub_verb_obj[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
