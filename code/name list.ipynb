{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import chardet\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def main():\n",
    "    dir = \"/Users/janine/Documents/NLP/Project/code/text_data/\"\n",
    "    raw_text_data = read_file(dir)\n",
    "    data_before_token, processed_text_data = preprocessing(raw_text_data)\n",
    "    #print(processed_text_data['Harry Potter and the Prisoner of Azkaban'])\n",
    "    \n",
    "    return data_before_token, processed_text_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(dir):\n",
    "    json_file = Path('{}Raw-Harry-Potter-Series.json'.format(dir))\n",
    "    book = {}\n",
    "    if not json_file.exists():\n",
    "        files = []\n",
    "        pattern = '\\.txt'\n",
    "        for readfile in os.listdir(dir):\n",
    "            if re.search(pattern, readfile):\n",
    "                files.append(readfile)\n",
    "        pattern = '(.+)\\..+$'\n",
    "        for file_name in files:\n",
    "            decode_text = []\n",
    "            with open(dir + file_name, 'rb') as f_read:\n",
    "                text = f_read.readlines()\n",
    "                for line in text:\n",
    "                    type = chardet.detect(line)\n",
    "                    line = line.decode(type[\"encoding\"])\n",
    "                    decode_text.append(line)\n",
    "                file_name = re.findall(pattern, file_name)[0]\n",
    "                book[file_name] = decode_text\n",
    "            f_read.close()\n",
    "\n",
    "        ##TODO:Save book to json file\n",
    "        with open(dir + 'Raw-Harry-Potter-Series.json', 'a') as outfile:\n",
    "            json.dump(book, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "    else:\n",
    "        readfile = open('{}Raw-Harry-Potter-Series.json'.format(dir),\"r\")\n",
    "        book = eval(readfile.read())\n",
    "    return book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    processed_data = {}\n",
    "    data_before_token = {}\n",
    "    \n",
    "    for key in data.keys():\n",
    "        text_of_book = data[key]\n",
    "        pattern = r'^[ยก]*'\n",
    "        new_sentence = []\n",
    "        for sentence in text_of_book:\n",
    "            sentence = sentence.strip('\\n')\n",
    "            sentence = re.sub(pattern, '', sentence)\n",
    "            new_sentence.append(sentence)\n",
    "            \n",
    "        data_before_token[key] = new_sentence\n",
    "\n",
    "        sentence_word_list = []\n",
    "        \n",
    "        ##Tokenization\n",
    "        for sentence in new_sentence:\n",
    "            sentence_word_list.append(word_tokenize(sentence))\n",
    "            \n",
    "        processed_data[key] = sentence_word_list\n",
    "\n",
    "        \n",
    "        \n",
    "        ##Stemming\n",
    "        #new_stemming_sentence_word_list = []\n",
    "        #ps = PorterStemmer()\n",
    "\n",
    "        #for sentence_word in sentence_word_list:\n",
    "            #stemming_words = []\n",
    "            #for word in sentence_word:\n",
    "                #stemming_words.append(ps.stem(word))\n",
    "            #new_stemming_sentence_word_list.append(stemming_words)\n",
    "\n",
    "        ##Lemmatization\n",
    "        #new_lemmatized_sentence_word_list = []\n",
    "        #lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        #for sentence_word in new_stemming_sentence_word_list:\n",
    "            #lemmatized_words = []\n",
    "            #for word in sentence_word:\n",
    "                #lemmatized_words.append(lemmatizer.lemmatize(word))\n",
    "            #new_lemmatized_sentence_word_list.append(lemmatized_words)\n",
    "        #processed_data[key] = new_lemmatized_sentence_word_list\n",
    "\n",
    "    return data_before_token, processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    data_before_token, processed_text_data = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['CHAPTER', 'ONE'], ['THE', 'BOY', 'WHO', 'LIVED']]\n",
      "['CHAPTER ONE', 'THE BOY WHO LIVED', \"Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense.\"]\n"
     ]
    }
   ],
   "source": [
    "print(processed_text_data['Harry Potter and the Sorcerer Stone'][:2])\n",
    "list1=data_before_token['Harry Potter and the Sorcerer Stone'][:3]                                 \n",
    "print(list1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the characters name\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_name = {}#name list for each book\n",
    "\n",
    "for key in data_before_token.keys():\n",
    "    text = data_before_token[key]\n",
    "    name = []\n",
    "    \n",
    "    for sentence in text:       \n",
    "        doc = nlp(sentence)\n",
    "        for X in doc.ents:\n",
    "            if X.label_ == 'PERSON':\n",
    "                if X.text not in name: #avoid overlapping\n",
    "                    name.append(X.text)\n",
    "            \n",
    "    all_name[key] = name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Harry Potter and The Chamber Of Secrets', 'Harry Potter and the Deathly Hallows', 'Harry Potter and the Goblet of Fire', 'Harry Potter and The Half-Blood Prince', 'Harry Potter and the Order of the Phoenix', 'Harry Potter and the Prisoner of Azkaban', 'Harry Potter and the Sorcerer Stone'])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_name.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first eposide\n",
    "len(all_name['Harry Potter and the Sorcerer Stone'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dursley', 'Potter', 'Harry', 'Next Door', 'Jim McGuffin', 'Jim', 'Ted', 'Yorkshire', 'Dundee', 'Mysterious', 'Howard', 'Albus Dumbledore', 'the Put-Outer', 'McGonagall', 'Muggle', 'Dumbledore', 'Voldemort', 'Pomfrey', 'Lily', 'James Potter', 'James', 'Albus', 'Harry Potter', 'Bristol', 'Duddy', 'Dudley', 'Scotch', 'Uncle Vernon', 'Marge', 'Atta', 'Figg', 'Snowy', 'Paws', 'Majorca', 'Mummy', 'Piers Polkiss', 'Piers', 'MOTORCYCLES', \"Uncle Vernon's\", 'Boa Constrictor', 'Dennis', 'Malcolm', 'Gordon', 'Harry Hunting', 'Ickle Dudleykins', 'H. Potter', 'Little Whinging', 'Surrey', 'H.', 'Spray', 'Yeh', 'Grounds', 'Hogwarts', 'Keeper', 'Keys', 'Headmaster', 'Wizardry', 'Minerva McGonagall', 'Gallopin', 'Gorgons', 'Knew', 'Blimey', 'Nah', 'Don', 'Horribly', \"Reckon Dumbledore's\", 'McKinnons', 'Disappeared', \"James Potter' s\", 'Yer', 'ALBUS- DUMBLEDORE-', 'FRONT-', \"Shouldn'ta\", 'Tap', 'Mm', 'Gringotts', 'Run', 'Goblins', 'Fer Dumbledore', 'Miranda Goshawk', 'Adalbert Waffling', 'Drafts', 'Arsenius Jigger', 'Tom', 'The Leaky Cauldron', 'Doris Crockford', 'Quirrell', 'Pewter', 'Sickles', 'Screech', 'Barn', 'Brown', \"Yeh'd\", 'Stalagmite', 'Malkin', 'Quidditch', 'Hufflepuff', 'Hair Loss', 'Jelly-Legs', 'Vindictus Viridian', 'Fine Wands', 'Ollivander', 'Yew', 'Leaky Cauldron', 'Hedwig', 'Diagon Alley', 'Percy', 'George', 'Fred', 'Ron', 'Gran', 'Lee', \"Lee Jordan's\", 'George Weasley', 'Bye', 'Mom', 'Weasleys', 'Wish', 'Bill', 'Charlie', \"Bettie Bott's\", 'Drooble', 'Best Blowing Gum', 'Cauldron Cakes', 'Licorice Wands', 'Agrippa', 'HEADMASTER', 'Grindelwald', 'Nicolas Flamel', 'Morgana', 'Merlin', 'Cliodna', \"Bertie Bott's\", 'Turn', 'Hermione Granger', 'Ron Weasley', 'Stupid', 'Gloom', 'Dad', 'Ravenclaw', \"Harry Potter's\", 'Crabbe', 'Malfoy', 'Draco Malfoy', 'Ron - Ron', 'Said', 'Ye', 'Set Gryffindors', 'Abbott', 'Hannah', 'Susan', 'Ravenclaws', 'Terry', 'Lavender', 'Finnigan', 'Seamus', 'Granger', 'Sally-Anne', 'Percy the Prefect', 'Weasley', 'Quirtell', 'Thomas', 'Dean', 'Lisa', 'Percy Weasley', 'Zabini', 'Blaise', 'Nicholas de Mimsy-Porpington', 'Nicholas de Mimsy', 'Seamus Finnigan', 'Nicholas', \"The Bloody Baron's\", 'Blocks', 'Filch', 'Hooch', 'Hoggy Warty Hogwarts', 'Password', 'Nick', 'Norris', 'Slytherins', 'Wish McGonagall', 'Dear Harry', 'Snape', 'Tut', 'Cheer', 'Fang', 'Hams', 'Fitch', 'GRINGOTTS BREAK-IN LATEST', 'Malfay', 'Muggles', 'Dean Thomas', 'Neville', \"Hermione Granger's\", 'Pansy Parkinson', 'Longbottom', 'Wood', 'Oliver Wood', 'Charlie Weasley', 'Lee Jordan', 'Bet', 'Gregory', 'Wizard', 'Wands', \"Malfoy'll\", 'Charms', 'CHARMS', 'Alohomora', 'Sha', 'Told', 'Haaaaaa', 'Nimbus', 'Wizard Baruffio', 'Levi-o', 'Parvati Patil', 'Peeves', 'Hermlone', 'Quidditch Through', 'West Ham', 'Chaser Angelina Johnson', 'Fred Weasley', 'Oliver', 'Marcus Flint', 'Angelina Johnson', 'Chaser', \"Oliver Wood's\", 'Johnson', 'Adrian Pucey', 'Bludger', 'Keeper Bletchley', 'Slytherin Seeker Terence Higgs', 'Higgs', 'Jordan', 'Harry swung', 'Flamel', 'NMat', 'G.', 'Gred', 'Excitement', 'Ronald Weasley', 'Slytherin', \"D'you\", \"Nicolas Flamel'\", 'Sorcerer', 'Locomotor Mortis', 'Gryffindor', 'Gliding', 'Pomftey', 'Fluffy', \"Dragon Keeper's\", 'Ridgeback', 'Wonder', 'Norbert', 'Dear Ron', 'Bye-bye', 'Add Norbert', 'Miss Granger', 'Fifty', 'Everywhere Harry', 'McGonagall Harry', 'Hurry', 'Bin', 'bin', 'BEHIND', 'Bane', 'Firenze', \"Bane'll\", 'Eager', 'Mighta bin', 'Baron', 'Peevsie', 'Devil', 'Seeker', 'Stone', 'The Stone', 'Tokens', 'Misters Fred', 'Nicolas', 'Bettie Bott', 'Ear', 'Neville Longbottom', 'Ginny Weasley', 'Harry hung back']\n"
     ]
    }
   ],
   "source": [
    "print(all_name['Harry Potter and the Sorcerer Stone'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#save all names into one json file\n",
    "with open('name_list'+'.json','a') as outfile:\n",
    "    json.dump(all_name,outfile,ensure_ascii=False)\n",
    "    outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save name list for each eposide\n",
    "for key in all_name.keys():\n",
    "    content = all_name[key]\n",
    "    file = open('name_list-'+str(key)+'.txt','w')\n",
    "    file.write(str(content));\n",
    "    file.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
